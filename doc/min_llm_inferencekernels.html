<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_shg7cn1k2uso-8{list-style-type:none}ol.lst-kix_pqd71os6zwvu-7.start{counter-reset:lst-ctn-kix_pqd71os6zwvu-7 0}ul.lst-kix_huhj911g8v0o-8{list-style-type:none}ul.lst-kix_huhj911g8v0o-7{list-style-type:none}ol.lst-kix_pqd71os6zwvu-4.start{counter-reset:lst-ctn-kix_pqd71os6zwvu-4 0}.lst-kix_huhj911g8v0o-7>li:before{content:"\0025cb   "}ul.lst-kix_lfu3nelbeahd-8{list-style-type:none}ul.lst-kix_lfu3nelbeahd-7{list-style-type:none}ul.lst-kix_lfu3nelbeahd-6{list-style-type:none}ul.lst-kix_lfu3nelbeahd-5{list-style-type:none}.lst-kix_huvg31y7yiez-8>li:before{content:"\0025a0   "}.lst-kix_huhj911g8v0o-8>li:before{content:"\0025a0   "}ul.lst-kix_lfu3nelbeahd-4{list-style-type:none}ul.lst-kix_shg7cn1k2uso-4{list-style-type:none}ul.lst-kix_shg7cn1k2uso-5{list-style-type:none}ul.lst-kix_shg7cn1k2uso-6{list-style-type:none}ul.lst-kix_shg7cn1k2uso-7{list-style-type:none}ul.lst-kix_shg7cn1k2uso-0{list-style-type:none}ul.lst-kix_shg7cn1k2uso-1{list-style-type:none}ul.lst-kix_shg7cn1k2uso-2{list-style-type:none}ul.lst-kix_shg7cn1k2uso-3{list-style-type:none}.lst-kix_shg7cn1k2uso-4>li:before{content:"\0025cb   "}ol.lst-kix_pqd71os6zwvu-1.start{counter-reset:lst-ctn-kix_pqd71os6zwvu-1 0}.lst-kix_huvg31y7yiez-3>li:before{content:"\0025cf   "}.lst-kix_huvg31y7yiez-5>li:before{content:"\0025a0   "}.lst-kix_shg7cn1k2uso-3>li:before{content:"\0025cf   "}.lst-kix_huvg31y7yiez-0>li:before{content:"\0025cf   "}.lst-kix_huvg31y7yiez-4>li:before{content:"\0025cb   "}.lst-kix_huhj911g8v0o-0>li:before{content:"\0025cf   "}.lst-kix_shg7cn1k2uso-0>li:before{content:"\0025cf   "}.lst-kix_huvg31y7yiez-7>li:before{content:"\0025cb   "}.lst-kix_r73n3mn52s8o-6>li:before{content:"\0025cf   "}.lst-kix_shg7cn1k2uso-2>li:before{content:"\0025a0   "}.lst-kix_shg7cn1k2uso-1>li:before{content:"\0025cb   "}.lst-kix_huvg31y7yiez-6>li:before{content:"\0025cf   "}.lst-kix_huhj911g8v0o-6>li:before{content:"\0025cf   "}.lst-kix_r73n3mn52s8o-5>li:before{content:"\0025a0   "}ul.lst-kix_huhj911g8v0o-4{list-style-type:none}ul.lst-kix_rmze2bldb4x9-8{list-style-type:none}ul.lst-kix_huhj911g8v0o-3{list-style-type:none}ul.lst-kix_huhj911g8v0o-6{list-style-type:none}ul.lst-kix_vpt8pv3b2iz4-0{list-style-type:none}.lst-kix_huhj911g8v0o-5>li:before{content:"\0025a0   "}ul.lst-kix_huhj911g8v0o-5{list-style-type:none}ul.lst-kix_huhj911g8v0o-0{list-style-type:none}ul.lst-kix_vpt8pv3b2iz4-2{list-style-type:none}ul.lst-kix_vpt8pv3b2iz4-1{list-style-type:none}ul.lst-kix_huhj911g8v0o-2{list-style-type:none}ul.lst-kix_vpt8pv3b2iz4-4{list-style-type:none}ul.lst-kix_huhj911g8v0o-1{list-style-type:none}.lst-kix_huhj911g8v0o-4>li:before{content:"\0025cb   "}.lst-kix_r73n3mn52s8o-7>li:before{content:"\0025cb   "}ul.lst-kix_vpt8pv3b2iz4-3{list-style-type:none}ul.lst-kix_vpt8pv3b2iz4-6{list-style-type:none}ul.lst-kix_rmze2bldb4x9-0{list-style-type:none}ul.lst-kix_vpt8pv3b2iz4-5{list-style-type:none}ul.lst-kix_rmze2bldb4x9-1{list-style-type:none}ul.lst-kix_vpt8pv3b2iz4-8{list-style-type:none}ul.lst-kix_rmze2bldb4x9-2{list-style-type:none}.lst-kix_huvg31y7yiez-1>li:before{content:"\0025cb   "}.lst-kix_huhj911g8v0o-1>li:before{content:"\0025cb   "}.lst-kix_huhj911g8v0o-3>li:before{content:"\0025cf   "}.lst-kix_r73n3mn52s8o-8>li:before{content:"\0025a0   "}ul.lst-kix_vpt8pv3b2iz4-7{list-style-type:none}ul.lst-kix_rmze2bldb4x9-3{list-style-type:none}ul.lst-kix_rmze2bldb4x9-4{list-style-type:none}ul.lst-kix_rmze2bldb4x9-5{list-style-type:none}ul.lst-kix_rmze2bldb4x9-6{list-style-type:none}.lst-kix_huvg31y7yiez-2>li:before{content:"\0025a0   "}.lst-kix_huhj911g8v0o-2>li:before{content:"\0025a0   "}ul.lst-kix_rmze2bldb4x9-7{list-style-type:none}ol.lst-kix_pqd71os6zwvu-2.start{counter-reset:lst-ctn-kix_pqd71os6zwvu-2 0}ul.lst-kix_huvg31y7yiez-5{list-style-type:none}ul.lst-kix_huvg31y7yiez-4{list-style-type:none}ul.lst-kix_huvg31y7yiez-3{list-style-type:none}ul.lst-kix_huvg31y7yiez-2{list-style-type:none}ul.lst-kix_huvg31y7yiez-8{list-style-type:none}ul.lst-kix_huvg31y7yiez-7{list-style-type:none}ul.lst-kix_huvg31y7yiez-6{list-style-type:none}.lst-kix_shg7cn1k2uso-6>li:before{content:"\0025cf   "}.lst-kix_shg7cn1k2uso-5>li:before{content:"\0025a0   "}.lst-kix_pqd71os6zwvu-3>li{counter-increment:lst-ctn-kix_pqd71os6zwvu-3}.lst-kix_pqd71os6zwvu-6>li{counter-increment:lst-ctn-kix_pqd71os6zwvu-6}.lst-kix_shg7cn1k2uso-7>li:before{content:"\0025cb   "}.lst-kix_shg7cn1k2uso-8>li:before{content:"\0025a0   "}.lst-kix_rmze2bldb4x9-0>li:before{content:"\0025cf   "}ul.lst-kix_huvg31y7yiez-1{list-style-type:none}ol.lst-kix_pqd71os6zwvu-8.start{counter-reset:lst-ctn-kix_pqd71os6zwvu-8 0}ul.lst-kix_huvg31y7yiez-0{list-style-type:none}.lst-kix_lfu3nelbeahd-3>li:before{content:"\0025cf   "}.lst-kix_pqd71os6zwvu-0>li{counter-increment:lst-ctn-kix_pqd71os6zwvu-0}.lst-kix_rmze2bldb4x9-1>li:before{content:"\0025cb   "}.lst-kix_lfu3nelbeahd-2>li:before{content:"\0025a0   "}.lst-kix_lfu3nelbeahd-1>li:before{content:"\0025cb   "}.lst-kix_pqd71os6zwvu-0>li:before{content:"" counter(lst-ctn-kix_pqd71os6zwvu-0,decimal) ". "}.lst-kix_lfu3nelbeahd-0>li:before{content:"\0025cf   "}.lst-kix_pqd71os6zwvu-1>li:before{content:"" counter(lst-ctn-kix_pqd71os6zwvu-1,lower-latin) ". "}.lst-kix_pqd71os6zwvu-3>li:before{content:"" counter(lst-ctn-kix_pqd71os6zwvu-3,decimal) ". "}.lst-kix_pqd71os6zwvu-2>li:before{content:"" counter(lst-ctn-kix_pqd71os6zwvu-2,lower-roman) ". "}.lst-kix_pqd71os6zwvu-5>li:before{content:"" counter(lst-ctn-kix_pqd71os6zwvu-5,lower-roman) ". "}.lst-kix_pqd71os6zwvu-7>li:before{content:"" counter(lst-ctn-kix_pqd71os6zwvu-7,lower-latin) ". "}ol.lst-kix_pqd71os6zwvu-0.start{counter-reset:lst-ctn-kix_pqd71os6zwvu-0 0}.lst-kix_pqd71os6zwvu-4>li:before{content:"" counter(lst-ctn-kix_pqd71os6zwvu-4,lower-latin) ". "}.lst-kix_pqd71os6zwvu-8>li:before{content:"" counter(lst-ctn-kix_pqd71os6zwvu-8,lower-roman) ". "}.lst-kix_rmze2bldb4x9-8>li:before{content:"\0025a0   "}.lst-kix_pqd71os6zwvu-5>li{counter-increment:lst-ctn-kix_pqd71os6zwvu-5}.lst-kix_pqd71os6zwvu-6>li:before{content:"" counter(lst-ctn-kix_pqd71os6zwvu-6,decimal) ". "}.lst-kix_lfu3nelbeahd-4>li:before{content:"\0025cb   "}.lst-kix_rmze2bldb4x9-2>li:before{content:"\0025a0   "}.lst-kix_rmze2bldb4x9-4>li:before{content:"\0025cb   "}.lst-kix_lfu3nelbeahd-6>li:before{content:"\0025cf   "}.lst-kix_lfu3nelbeahd-5>li:before{content:"\0025a0   "}.lst-kix_rmze2bldb4x9-3>li:before{content:"\0025cf   "}.lst-kix_rmze2bldb4x9-7>li:before{content:"\0025cb   "}.lst-kix_lfu3nelbeahd-8>li:before{content:"\0025a0   "}ul.lst-kix_r73n3mn52s8o-5{list-style-type:none}ul.lst-kix_r73n3mn52s8o-4{list-style-type:none}ul.lst-kix_r73n3mn52s8o-7{list-style-type:none}ul.lst-kix_r73n3mn52s8o-6{list-style-type:none}.lst-kix_rmze2bldb4x9-6>li:before{content:"\0025cf   "}ul.lst-kix_r73n3mn52s8o-1{list-style-type:none}ul.lst-kix_r73n3mn52s8o-0{list-style-type:none}.lst-kix_lfu3nelbeahd-7>li:before{content:"\0025cb   "}ul.lst-kix_r73n3mn52s8o-3{list-style-type:none}.lst-kix_pqd71os6zwvu-4>li{counter-increment:lst-ctn-kix_pqd71os6zwvu-4}ul.lst-kix_r73n3mn52s8o-2{list-style-type:none}.lst-kix_rmze2bldb4x9-5>li:before{content:"\0025a0   "}ul.lst-kix_r73n3mn52s8o-8{list-style-type:none}ol.lst-kix_pqd71os6zwvu-3.start{counter-reset:lst-ctn-kix_pqd71os6zwvu-3 0}.lst-kix_pqd71os6zwvu-2>li{counter-increment:lst-ctn-kix_pqd71os6zwvu-2}ol.lst-kix_pqd71os6zwvu-1{list-style-type:none}ol.lst-kix_pqd71os6zwvu-0{list-style-type:none}ol.lst-kix_pqd71os6zwvu-3{list-style-type:none}ol.lst-kix_pqd71os6zwvu-2{list-style-type:none}ol.lst-kix_pqd71os6zwvu-5{list-style-type:none}ol.lst-kix_pqd71os6zwvu-4{list-style-type:none}.lst-kix_pqd71os6zwvu-8>li{counter-increment:lst-ctn-kix_pqd71os6zwvu-8}.lst-kix_r73n3mn52s8o-2>li:before{content:"\0025a0   "}.lst-kix_r73n3mn52s8o-4>li:before{content:"\0025cb   "}.lst-kix_vpt8pv3b2iz4-7>li:before{content:"\0025cb   "}.lst-kix_r73n3mn52s8o-3>li:before{content:"\0025cf   "}.lst-kix_vpt8pv3b2iz4-8>li:before{content:"\0025a0   "}ol.lst-kix_pqd71os6zwvu-7{list-style-type:none}.lst-kix_pqd71os6zwvu-7>li{counter-increment:lst-ctn-kix_pqd71os6zwvu-7}ol.lst-kix_pqd71os6zwvu-6{list-style-type:none}ol.lst-kix_pqd71os6zwvu-6.start{counter-reset:lst-ctn-kix_pqd71os6zwvu-6 0}.lst-kix_pqd71os6zwvu-1>li{counter-increment:lst-ctn-kix_pqd71os6zwvu-1}ol.lst-kix_pqd71os6zwvu-8{list-style-type:none}.lst-kix_r73n3mn52s8o-0>li:before{content:"\0025cf   "}.lst-kix_vpt8pv3b2iz4-5>li:before{content:"\0025a0   "}.lst-kix_r73n3mn52s8o-1>li:before{content:"\0025cb   "}.lst-kix_vpt8pv3b2iz4-6>li:before{content:"\0025cf   "}.lst-kix_vpt8pv3b2iz4-1>li:before{content:"\0025cb   "}.lst-kix_vpt8pv3b2iz4-0>li:before{content:"\0025cf   "}.lst-kix_vpt8pv3b2iz4-4>li:before{content:"\0025cb   "}ul.lst-kix_lfu3nelbeahd-3{list-style-type:none}ul.lst-kix_lfu3nelbeahd-2{list-style-type:none}ul.lst-kix_lfu3nelbeahd-1{list-style-type:none}ul.lst-kix_lfu3nelbeahd-0{list-style-type:none}.lst-kix_vpt8pv3b2iz4-3>li:before{content:"\0025cf   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_vpt8pv3b2iz4-2>li:before{content:"\0025a0   "}ol.lst-kix_pqd71os6zwvu-5.start{counter-reset:lst-ctn-kix_pqd71os6zwvu-5 0}ol{margin:0;padding:0}table td,table th{padding:0}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c8{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c11{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c17{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c15{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:italic}.c2{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c3{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c18{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c0{padding:0;margin:0}.c5{margin-left:72pt;padding-left:0pt}.c19{height:14pt}.c12{font-style:italic}.c14{padding-left:0pt}.c10{font-weight:700}.c7{margin-left:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c18 doc-content"><h1 class="c17" id="h.emiyeljbwf5b"><span class="c13">High level design</span></h1><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 287.11px;"><img alt="" src="images/image17.png" style="width: 624.00px; height: 287.11px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c6"></span></p><h1 class="c17" id="h.nxe9jvn28jpi"><span class="c13">Detailed Design</span></h1><h2 class="c15" id="h.3td88fqx9zs"><span class="c16">Inferencer</span></h2><p class="c4"><span class="c6">This class receives one batch of tokens, and returns a batch of generated tokens.</span></p><h2 class="c15" id="h.6ml24l2sigxh"><span class="c16">Inference Model</span></h2><p class="c4"><span class="c6">We use pretty simple self-attention.</span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 81.33px;"><img alt="" src="images/image19.png" style="width: 624.00px; height: 81.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c6"></span></p><h3 class="c3" id="h.98cwrmjn0u4v"><span class="c11">Self-attention</span></h3><p class="c4"><span class="c6">The basic formula of the self-attention is:</span></p><p class="c4"><img src="images/image1.png"></p><p class="c1"><span class="c6"></span></p><p class="c4"><span>For the training case, Q, K, V are all </span><img src="images/image2.png"><span class="c6">. However, for inference, we actually only need to do the inference for the last sequence of each batch.</span></p><p class="c4"><span class="c6">Meanwhile, we want to keep the cache for K, V.</span></p><p class="c1"><span class="c6"></span></p><p class="c4"><span class="c6">We have 11 related tensors:</span></p><ul class="c0 lst-kix_rmze2bldb4x9-0 start"><li class="c4 c14 c7 li-bullet-0"><span>inp: Input tensor of shape </span><img src="images/image3.png"></li><li class="c4 c14 c7 li-bullet-0"><span>lengths: The sequence length of each batch. </span><img src="images/image4.png"></li><li class="c4 c14 c7 li-bullet-0"><span>wk, wq, wv: </span><img src="images/image5.png"></li><li class="c4 c14 c7 li-bullet-0"><span>new_batch_idx: the index of the new batches. -1 means this index is not new. </span><img src="images/image4.png"></li><li class="c4 c14 c7 li-bullet-0"><span>kt_cache: k cache. But we actually always use its transpose. </span><img src="images/image6.png"></li><li class="c4 c14 c7 li-bullet-0"><span>v_cahce: v_cache. </span><img src="images/image7.png"></li><li class="c4 c7 c14 li-bullet-0"><span>q_output: The result of </span><img src="images/image8.png"><span>. </span><img src="images/image9.png"></li><li class="c4 c14 c7 li-bullet-0"><span>qkt_output: The result of </span><img src="images/image10.png"><span>. We also use it to store the softmax result. </span><img src="images/image11.png"></li><li class="c4 c14 c7 li-bullet-0"><span>attention_result: The result of the self-attention block. </span><img src="images/image9.png"></li></ul><p class="c1 c7"><span class="c6"></span></p><p class="c4"><span class="c6">We split the responsibilities into 5 cuda kernels.</span></p><h4 class="c2" id="h.bf0w2us5p967"><span class="c8">fill_new_kt_v_cache</span></h4><p class="c4"><span class="c6">This kernel checks the new_batch_idx and inserts into the kt_cache and v_cache accordingly.</span></p><ul class="c0 lst-kix_huvg31y7yiez-0 start"><li class="c4 c14 c7 li-bullet-0"><span>Launch </span><img src="images/image12.png"><span class="c10">&nbsp;</span><span class="c6">threads.</span></li><li class="c4 c14 c7 li-bullet-0"><span>If </span><span class="c12">new_batch_idx[blockIdx.z] </span><span>is not -1, we use </span><span class="c12">inp[i_batch, 1:lengths[i_batch], input_dim] </span><span>to multiply with </span><span class="c9">wk, wv.</span></li><li class="c4 c14 c7 li-bullet-0"><span>Update </span><span class="c12">new_batch_idx[blockIdx.z] </span><span class="c6">to -1</span></li><li class="c4 c14 c7 li-bullet-0"><span>Insert the result into </span><span class="c12">kt_cache </span><span>and </span><span class="c9">v_cache.</span></li></ul><h4 class="c2" id="h.hr442utkqit0"><span class="c8">get_latest_kt_q_v</span></h4><p class="c4"><span>This kernel multiplies each batch&rsquo;s latest embedding with </span><span class="c12">wk, wq</span><span>&nbsp;and </span><span class="c12">wv</span><span>. And set to </span><span class="c12">kt_cache, v_cache </span><span>and </span><span class="c9">q_output.</span></p><h4 class="c2" id="h.dq2atvkn19dy"><span class="c8">qkt</span></h4><p class="c4"><span>This kernel multiples </span><span class="c12">q </span><span>with </span><span class="c12">kt. </span><span>And the </span><span class="c12">lengths </span><span class="c6">is used to avoid necessary computation.</span></p><h4 class="c2" id="h.cp89khrib8ac"><span class="c8">softmax_in_place_with_lengths</span></h4><p class="c4"><span>Apply softmax_in_place operation for </span><span class="c12">qkt_output. </span><span>Note any element exceeding the </span><span class="c12">lengths[i_batch] </span><span class="c6">should be 0.</span></p><h4 class="c2" id="h.m2hcq4z2vz94"><span class="c8">softmax_v</span></h4><p class="c4"><span>This kernel multiplies the </span><span class="c12">qkt_output </span><span>with </span><span class="c12">v_cache, </span><span>and saves into </span><span class="c9">attention_result.</span></p><h2 class="c15" id="h.edn4zqrmcs9x"><span class="c16">Item Processor</span></h2><p class="c4"><span>The </span><span class="c12">ItemProcessor </span><span class="c6">manages the items from decoding and to be encoded.</span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 420.82px; height: 462.18px;"><img alt="" src="images/image18.png" style="width: 420.82px; height: 462.18px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c3 c19" id="h.czih9txduzoz"><span class="c11"></span></h3><h3 class="c3" id="h.z0ptmy4fda3n"><span class="c11">Decoder</span></h3><p class="c4"><span class="c6">The decoder serves 2 tasks:</span></p><ul class="c0 lst-kix_huhj911g8v0o-0 start"><li class="c4 c14 c7 li-bullet-0"><span class="c6">On cuda kernels:</span></li></ul><ul class="c0 lst-kix_huhj911g8v0o-1 start"><li class="c4 c5 li-bullet-0"><span>Given the embeddings of </span><img src="images/image9.png"><span>. The kernel multiplies it with the </span><span class="c12">embedding_table </span><span>to get similarity of </span><img src="images/image13.png"><span class="c6">.</span></li><li class="c4 c5 li-bullet-0"><span>Considering the </span><span class="c12">vocab </span><span>can be large. We use one block to handle per row, to find the maximum index. Here, we ignore the softmax operation. The result is of shape </span><img src="images/image4.png"><span>. Meanwhile, in the same kernel, we can store the embedding of this token (</span><img src="images/image14.png"><span>) to the </span><span class="c12">inp </span><span>of shape </span><img src="images/image3.png"><span>, given the </span><span class="c12">lengths.</span><span class="c6">&nbsp;This can work because we are using greedy sampling. We have to change this strategy if we use beam search in the future.</span></li><li class="c4 c5 li-bullet-0"><span>If </span><span class="c12">lengths[i_batch_index] == 0, </span><span class="c6">this is one invalid row. Return -1.</span></li></ul><h3 class="c3" id="h.jlpkvw2102sa"><span class="c11">Processing Items</span></h3><h4 class="c2" id="h.oetf74rmvmcx"><span>process_decoder_result</span></h4><ol class="c0 lst-kix_pqd71os6zwvu-0 start" start="1"><li class="c4 c14 c7 li-bullet-0"><span>Copy the tokens of </span><img src="images/image4.png"><span class="c6">&nbsp;to cpu. Find the corresponding tokens and append to pending results.</span></li><li class="c4 c14 c7 li-bullet-0"><span class="c6">For each item, check</span></li></ol><ol class="c0 lst-kix_pqd71os6zwvu-1 start" start="1"><li class="c4 c5 li-bullet-0"><span>If the token_index is -1, this is one empty row. But still add it to </span><span class="c12">finished_indices </span><span class="c6">because we can add new_items to this row.</span></li><li class="c4 c5 li-bullet-0"><span>If the token is &lt;EOF&gt; or the length exceeds the maximum, add to </span><span class="c12">finished_items_ </span><span>and </span><span class="c9">finished_indices.</span></li></ol><h4 class="c2" id="h.c1sam44y77w1"><span class="c8">insert_new_items</span></h4><ul class="c0 lst-kix_vpt8pv3b2iz4-0 start"><li class="c4 c14 c7 li-bullet-0"><span>If </span><span class="c12">finished_indices.size() &gt; 0</span><span class="c6">, try to fetch this amount of </span></li></ul><h3 class="c3" id="h.qlw7rx01ul8"><span class="c11">Encoder</span></h3><p class="c4"><span>The encoder only encodes the new items of shape </span><img src="images/image15.png"><span>&nbsp;with </span><span class="c9">lengths:</span></p><ul class="c0 lst-kix_lfu3nelbeahd-0 start"><li class="c4 c14 c7 li-bullet-0"><span>Calculates the embeddings of shape </span><img src="images/image16.png"><span>&nbsp;and clone to the corresponding </span><span class="c12">new_batch_idx </span><span>of </span><span class="c9">inp.</span></li></ul><p class="c1"><span class="c6"></span></p><h2 class="c15" id="h.4p71pfc6wecy"><span class="c16">Extend to other structures</span></h2><p class="c4"><span class="c6">This optimization idea can easily apply to any layer:</span></p><ul class="c0 lst-kix_r73n3mn52s8o-0 start"><li class="c4 c14 c7 li-bullet-0"><span class="c6">Store the cache result for past sequences.</span></li><li class="c4 c14 c7 li-bullet-0"><span>For new items, with </span><span class="c12">new_items </span><span>and </span><span class="c12">new_lengths</span><span>, calculate the cache once, and add to the existing cache.</span></li></ul><h1 class="c17" id="h.qb0koprayet3"><span class="c13">Future plans</span></h1><ul class="c0 lst-kix_shg7cn1k2uso-0 start"><li class="c4 c14 c7 li-bullet-0"><span>Inferencer can keep receiving new tokens. We can have a thread to put the new tokens to the </span><span class="c10">NewItems.</span></li></ul></body></html>
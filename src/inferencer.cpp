#include "inferencer.h"
#include "items_storage.h"
#include "tensor.hpp"
#include <string>
#include <vector>

constexpr int N_VOCAB = 20480;
constexpr int DIMS = 1024;

void start_inference_engine() {
    // ItemsHandler itemsHandler;
    // TensorFloat embedding_table({N_VOCAB, DIMS}, DeviceType::DEVICE);
    // std::vector<std::string> id_to_string;
    // std::vector<int> finished_indices;  // fill from 0 to n_batches



    // while (!itemsHandler.is_finished()) {
    //     insert_new_items
    //     encoder_launch
    //     result = model.forward()
    //     decoder_result = decoder
    //     finished_indices = process_decoder_result
    // }

}
